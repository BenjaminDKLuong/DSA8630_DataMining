{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Text preprocessing is an important step in the use of unstructured text documents for any type of data mining, information retrieval, or text analytics.\n",
    "This lab walks through the use of the Python Natural Language Toolkit (NLTK) to discuss the tools available for text preprocessing.\n",
    "Specifically, we are looking at the concepts of\n",
    "  1. Stop Words\n",
    "  1. Stemming\n",
    "  1. Lemmatization\n",
    "  \n",
    "In the labs after this, these things will be automatically handled for us as we build upon information retrieval.\n",
    "However, these are still key concepts to see in action.\n",
    "You will see them again as we continue to move forward with our text analytics in future modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Text documents often contain many occurrences of the same word. \n",
    "For example, in a document written in *English*, words such as *a, the, of, and it* are likely to occur very frequently. \n",
    "When classifying a document based on the number of times specific words occur in the text document, \n",
    "these words can lead to biases, especially since they are generally common in **all** text documents you might want to classify. \n",
    "As a result, the concept of [_stop words_](https://en.wikipedia.org/wiki/Stop_words) was invented. \n",
    "Basically, these words are the most commonly occurring words that should be removed during the tokenization process in order to improve subsequent text analytics efforts. \n",
    "\n",
    "We can easily specify that the __English__ stop words should be excluded during tokenization by using the `stop_words`. \n",
    "Note, _stop word_ dictionaries for other languages, or even specific domains, exist and can be used instead. \n",
    "We demonstrate the removal of stop words by using a `CountVectorizer` in the following simple example and compare it to using `CountVectorizer` without stop words removed.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "['this', 'module', 'introduced', 'many', 'concepts', 'in', 'text', 'analysis']\n",
      "\n",
      "Tokenization (with Stop words):\n",
      "['module', 'introduced', 'concepts', 'text', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# Define our vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word', lowercase=True)\n",
    "\n",
    "# Sample sentence to tokenize\n",
    "my_text = 'This module introduced many concepts in text analysis.'\n",
    "\n",
    "cv1 = CountVectorizer(lowercase=True)\n",
    "cv2 = CountVectorizer(stop_words = 'english', lowercase=True)\n",
    "\n",
    "tk_func1 = cv1.build_analyzer()\n",
    "tk_func2 = cv2.build_analyzer()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "print('Tokenization:')\n",
    "pp.pprint(tk_func1(my_text))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Tokenization (with Stop words):')\n",
    "pp.pprint(tk_func2(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "--- \n",
    "## Stemming\n",
    "\n",
    "\n",
    "We have looked at the removal of redundant or unimportant words, i.e., _stop words_. \n",
    "However, an issue still exists because of different word forms of the same base term; for example compute, computer, computed, and computing. \n",
    "The process of changing words back to their root term or basic form (by removing prefixes and suffixes) so that token frequencies match the use of the root token rather than being spread across multiple similar tokens is known as [stemming](https://en.wikipedia.org/wiki/Stemming). \n",
    "\n",
    "The most widely used stemmer, or program/method that performs stemming, is the _Porter Stemmer_, which was originally published in 1980 by Martin Porter. \n",
    "An improved version was released in 2000, which fixed a number of errors. \n",
    "NLTK includes the Porter Stemmer.\n",
    "This is used by creating a special function that tokenizes text documents and then passes this function as an argument to the `CountVectorizer` via the `tokenizer` attribute. \n",
    "By performing stemming inside this tokenize method, we can return a set of tokens for a document that have been stemmed. \n",
    "In the following code cell, we use a custom `tokenize` method that first builds a list of tokens by using nltk, and then maps the Porter Stemmer to the list of tokens to generate a stemmed list.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## See how the PorterStemmer behaves with a list of example words\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for w in example_words:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "be\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n"
     ]
    }
   ],
   "source": [
    "## See how the PorterStemmer works with tokenizing on a very pythony sentence\n",
    "\n",
    "new_text = \"It is important to be very pythonly while you are pythoning with python. \\\n",
    "All pythoners have pythoned poorly at least once.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(new_text)\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "for w in tokens:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Lemmatization\n",
    "\n",
    "\n",
    "In linguistics, lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. \n",
    "By \"inflected\" we mean the form of a word has been changed to express a particular grammatical function or attribute, typically tense, mood, person, number, case, and gender.\n",
    "\n",
    "In computational linguistics, lemmatization is the algorithmic process of determining the lemma for a given word. \n",
    "The process may involve complex tasks such as understanding context and determining the part of speech of a word in a sentence, which requires knowledge of the grammar of a language.\n",
    "\n",
    "In many languages, words appear in several inflected forms. \n",
    "For example, in English, the verb ‘to walk’ may appear as ‘walk’, ‘walked’, ‘walks’, ‘walking’. \n",
    "The base form, ‘walk’, that one might look up in a dictionary, is called the lemma for the word. \n",
    "\n",
    "Lemmatization is closely related to stemming. \n",
    "The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. \n",
    "However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to understand the difference between **Stemming** and **Lemmatization**. Look at the ouput for the following code, and then read the following explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem going: go\n",
      "Stem gone: gone\n",
      "Stem goes: goe\n",
      "Stem went: went\n",
      "\n",
      "\n",
      "Without context\n",
      "Lemmatise going: going\n",
      "Lemmatise gone: gone\n",
      "Lemmatise goes: go\n",
      "Lemmatise went: went\n",
      "\n",
      "\n",
      "With context\n",
      "Lemmatise going: go\n",
      "Lemmatise gone: go\n",
      "Lemmatise goes: go\n",
      "Lemmatise went: go\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "print(\"Stem %s: %s\" % (\"going\", stemmer.stem(\"going\")))\n",
    "print(\"Stem %s: %s\" % (\"gone\", stemmer.stem(\"gone\")))\n",
    "print(\"Stem %s: %s\" % (\"goes\", stemmer.stem(\"goes\")))\n",
    "print(\"Stem %s: %s\" % (\"went\", stemmer.stem(\"went\")))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Without context\")\n",
    "print(\"Lemmatise %s: %s\" % (\"going\", lemmatizer.lemmatize(\"going\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"gone\", lemmatizer.lemmatize(\"gone\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"goes\", lemmatizer.lemmatize(\"goes\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"went\", lemmatizer.lemmatize(\"went\")))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"With context\")\n",
    "print(\"Lemmatise %s: %s\" % (\"going\", lemmatizer.lemmatize(\"going\", pos=\"v\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"gone\", lemmatizer.lemmatize(\"gone\", pos=\"v\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"goes\", lemmatizer.lemmatize(\"goes\", pos=\"v\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"went\", lemmatizer.lemmatize(\"went\", pos=\"v\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the stemming process does not generate a real word, but a root form. \n",
    "On the other side, the lemmatizer generates real words, \n",
    "but without contextual information it is not able to distinguish between nouns and verbs. \n",
    "Hence the lemmatization process doesn’t change the word. \n",
    "\n",
    "The context is provided by the POS tag (\"v\" for verb in this example). \n",
    "We cannot specify POS tag everytime in order to lemmatize words in a text. \n",
    "NLTK generates POS tags automatically, using a simple function `pos_tag()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('sentence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "s = \"This is a simple sentence\"\n",
    "tokens = word_tokenize(s) # Generate list of tokens\n",
    "tokens_pos = pos_tag(tokens) \n",
    " \n",
    "print(tokens_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the `pos_tag()` function generates keywords for each word in the text. \n",
    "The outputs 'DT', 'VBZ', etc. are `tags` representing parts of speech from the [Penn Treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point\n",
    "Stop Words, Stemming, and Lemmatization are important pre-processing steps in text analytics applications. \n",
    "You can leverage the off-the-shelf solutions offered by NLTK into yout text analysis applications.\n",
    "Additionally, many code libraries and applications that perform more advanced text analytical processes incorporate these techniques in them by default.\n",
    "\n",
    "Below is some practice coding for you to experiment with the NLTK functionality above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "StringAction = \"We are meeting\"\n",
    "StringNoun =  \"We had a meeting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "'We are meeting':\n",
      "['meeting']\n",
      "  ... vs ...  \n",
      "Tokenization:\n",
      "'We had a meeting':\n",
      "['meeting']\n"
     ]
    }
   ],
   "source": [
    "# 1) Compare the result of parsing the two \n",
    "# variables, StringAction and StringNoun \n",
    "# using the tokenizer with the english stop words removed\n",
    "# ----------------------------------------\n",
    "\n",
    "# Import libraries\n",
    "import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Building the vectorizing tokenizer\n",
    "cv = CountVectorizer(stop_words = 'english', lowercase=True)\n",
    "tk_function = cv.build_analyzer()\n",
    "\n",
    "# Print out the comparison of stop-word enabled tokenizing the two variables\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "print('Tokenization:')\n",
    "print(\"'{}':\".format(StringAction))\n",
    "# -------- EDIT NEXT LINE --------\n",
    "pp.pprint(tk_function(StringAction))\n",
    "\n",
    "print(\"  ... vs ...  \")\n",
    "\n",
    "print('Tokenization:')\n",
    "print(\"'{}':\".format(StringNoun))\n",
    "# -------- EDIT NEXT LINE --------\n",
    "pp.pprint(tk_function(StringNoun))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'We are meeting':\n",
      "meet\n",
      "    \n",
      "  ... vs ...  \n",
      "    \n",
      "'We had a meeting':\n",
      "meet\n"
     ]
    }
   ],
   "source": [
    "# 2) Compare the result of parsing the two \n",
    "# variables, StringAction and StringNoun \n",
    "# using the stemmer with the english stop words removed\n",
    "# ----------------------------------------\n",
    "\n",
    "# Import libraries\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "### Add your code below to parse and stem the two variables\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "Action_tokens = nltk.word_tokenize(StringAction.lower())\n",
    "Action_tokens = [token for token in Action_tokens if token not in stop]\n",
    "\n",
    "Noun_tokens = nltk.word_tokenize(StringNoun.lower())\n",
    "Noun_tokens = [token for token in Noun_tokens if token not in stop]\n",
    "\n",
    "print(\"'{}':\".format(StringAction))\n",
    "for w in Action_tokens:\n",
    "    print(stemmer.stem(w))\n",
    "\n",
    "print(\"    \")\n",
    "print(\"  ... vs ...  \")\n",
    "print(\"    \")\n",
    "\n",
    "\n",
    "print(\"'{}':\".format(StringNoun))\n",
    "for w in Noun_tokens:\n",
    "    print(stemmer.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization\n",
      "----------------------------------------\n",
      "'We are meeting':\n",
      "No pos:  meeting\n",
      "With pos v:  meet\n",
      "    \n",
      "  ... vs ...  \n",
      "    \n",
      "'We had a meeting':\n",
      "No pos:  meeting\n",
      "With pos n:  meeting\n"
     ]
    }
   ],
   "source": [
    "# 3) Compare the result of parsing the two \n",
    "# variables, StringAction and StringNoun \n",
    "# using the lemmatization with the english stop words removed\n",
    "# ----------------------------------------\n",
    "\n",
    "# Import libraries\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "### Add your code below to parse and lemmatize the two variables\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "Action_tokens = [token for token in word_tokenize(StringAction.lower()) if token not in stop]\n",
    "Noun_tokens = [token for token in word_tokenize(StringNoun.lower()) if token not in stop]\n",
    "\n",
    "print(\"Lemmatization\")\n",
    "print(40*'-')\n",
    "print(\"'{}':\".format(StringAction))\n",
    "for w in Action_tokens:\n",
    "    print(\"No pos: \", lemmatizer.lemmatize(w))\n",
    "    # use part of speech verb\n",
    "    print(\"With pos v: \", lemmatizer.lemmatize(w, pos='v'))\n",
    "    \n",
    "print(\"    \")\n",
    "print(\"  ... vs ...  \")\n",
    "print(\"    \")\n",
    "\n",
    "print(\"'{}':\".format(StringNoun))\n",
    "for w in Noun_tokens:\n",
    "    print(\"No pos: \", lemmatizer.lemmatize(w))\n",
    "    # use part of speech noun\n",
    "    print(\"With pos n: \", lemmatizer.lemmatize(w, pos=\"n\"))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing in Full Text Search\n",
    "\n",
    "Now that we have seen these concepts in isolation, lets revisit the PostgreSQL Full Text Search.\n",
    "\n",
    "Specifically, you have loaded a new table that breaks each document from the book in the lab into individual lines.  \n",
    "You can review the load process for this data in [this notebook](./Practice_Load_BookLines.ipynb).\n",
    "The notebook includes a few queries, showing how the loaded lines may be a little more useful that just document matching.\n",
    "\n",
    "<span style=\"background-color:yellow\">For the commands below, replace the schema name `sebcq5` with your own pawprint.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "dsa_student=# select count(*),sum(length(line)) from sebcq5.booklines;\n",
    " count |   sum\n",
    "-------+---------\n",
    " 31259 | 4315223\n",
    "(1 row)\n",
    "```\n",
    "\n",
    "#### 31K lines\n",
    "\n",
    "#### Looking at a random line that was added:\n",
    "\n",
    "```SQL\n",
    "dsa_student=# \\x\n",
    "Expanded display is on.\n",
    "dsa_student=# select * from sebcq5.booklines where id = 31236;\n",
    "-[ RECORD 1 ]-+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "id            | 31236\n",
    "name          | /dsa/data/all_datasets/book/zeph.txt\n",
    "line_no       | 34\n",
    "line          | 2:14: And flocks shall lie down in the midst of her, all the beasts of the nations: both the cormorant and the bittern shall lodge in the upper lintels of it; their voice shall sing in the windows; desolation shall be in the thresholds: for he shall uncover the cedar work.\n",
    "line_tsv_gin  | '14':2 '2':1 'beast':15 'bittern':24 'cedar':51 'cormor':21 'desol':40 'flock':4 'lie':6 'lintel':30 'lodg':26 'midst':10 'nation':18 'shall':5,25,35,41,48 'sing':36 'threshold':45 'uncov':49 'upper':29 'voic':34 'window':39 'work':52\n",
    "line_tsv_gist | '14':2 '2':1 'beast':15 'bittern':24 'cedar':51 'cormor':21 'desol':40 'flock':4 'lie':6 'lintel':30 'lodg':26 'midst':10 'nation':18 'shall':5,25,35,41,48 'sing':36 'threshold':45 'uncov':49 'upper':29 'voic':34 'window':39 'work':52\n",
    "```\n",
    "\n",
    "Notice that we have built a document vector that is stemmed and has removed common (stop) wor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the line \n",
    "\n",
    "    2:14: And flocks shall lie down in the midst of her, \n",
    "    all the beasts of the nations: both the cormorant and \n",
    "    the bittern shall lodge in the upper lintels of it; \n",
    "    their voice shall sing in the windows; desolation shall \n",
    "    be in the thresholds: for he shall uncover the cedar work.\n",
    "\n",
    "Is tokenized into **_text search vector_ (tsv)**: \n",
    "```\n",
    "'14':2 '2':1 'beast':15 'bittern':24 'cedar':51 'cormor':21 \n",
    "'desol':40 'flock':4 'lie':6 'lintel':30 'lodg':26 'midst':10 \n",
    "'nation':18 'shall':5,25,35,41,48 'sing':36 'threshold':45 \n",
    "'uncov':49 'upper':29 'voic':34 'window':39 'work':52\n",
    "```\n",
    "\n",
    "Lets compare this line to the Python tokenizing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:14: And flocks shall lie down in the midst of her, all the beasts of the nations: both the cormorant and  the bittern shall lodge in the upper lintels of it;  their voice shall sing in the windows; desolation shall  be in the thresholds: for he shall uncover the cedar work.\n"
     ]
    }
   ],
   "source": [
    "line = \"2:14: And flocks shall lie down in the midst of her, \" + \\\n",
    "    \"all the beasts of the nations: both the cormorant and  \" + \\\n",
    "    \"the bittern shall lodge in the upper lintels of it;  \" + \\\n",
    "    \"their voice shall sing in the windows; desolation shall  \" + \\\n",
    "    \"be in the thresholds: for he shall uncover the cedar work.\"\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare processing of the line:\n",
    "\n",
    "In the cell below, use each of the Python techniques we discussed above to process the `line` variable.\n",
    "Then, answer the questions in the cells below the code to compare and contrast the Python methods versus the apparent techniques applied by PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14', 'flocks', 'shall', 'lie', 'midst', 'beasts', 'nations', 'cormorant', 'bittern', 'shall', 'lodge', 'upper', 'lintels', 'voice', 'shall', 'sing', 'windows', 'desolation', 'shall', 'thresholds', 'shall', 'uncover', 'cedar', 'work']\n",
      " \n",
      "Stem: \n",
      "['14', 'flock', 'shall', 'lie', 'midst', 'beast', 'nation', 'cormor', 'bittern', 'shall', 'lodg', 'upper', 'lintel', 'voic', 'shall', 'sing', 'window', 'desol', 'shall', 'threshold', 'shall', 'uncov', 'cedar', 'work']\n",
      " \n",
      "Lemma: \n",
      "['14', 'flock', 'shall', 'lie', 'midst', 'beast', 'nation', 'cormorant', 'bittern', 'shall', 'lodge', 'upper', 'lintel', 'voice', 'shall', 'sing', 'window', 'desolation', 'shall', 'threshold', 'shall', 'uncover', 'cedar', 'work']\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pprint\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 1) Add code to process the line variable with \n",
    "# Stop Word tokenization, Stemming, and Lemmatization\n",
    "# ---------------------------------------------\n",
    "\n",
    "#stop = stopwords.words('english')\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', lowercase=True)\n",
    "tk_function = cv.build_analyzer()\n",
    "\n",
    "line_tokens = tk_function(line)\n",
    "print(line_tokens)\n",
    "print(' ')\n",
    "stem = []\n",
    "stemmer = PorterStemmer()\n",
    "for w in line_tokens:\n",
    "    stem.append(stemmer.stem(w))\n",
    "\n",
    "print('Stem: ')\n",
    "print(stem)\n",
    "\n",
    "print(' ')\n",
    "print('Lemma: ')\n",
    "\n",
    "lemma = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w in line_tokens:\n",
    "    lemma.append(lemmatizer.lemmatize(w))\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 2) Identify the differences and/or similarities in the \n",
    "# removal of stop words from the `line` variable.\n",
    "\n",
    "The line variable has all the tokens in the order they appear in one string.\n",
    "\n",
    "After removal of stop word, the chapter:verse reference, punctuation, and the main words appear. It is a list of strings, rather than one string. The words removed include 'and', 'down', 'in', 'the', 'of', 'her', 'all', 'both', 'it', 'their', 'be', 'for', 'he'. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 3) Identify the differences and/or similarities in the \n",
    "# stemming of the `line` variable.\n",
    "\n",
    "After stemming the line with stop words removed, plurals are turned to singular ['flocks' to 'flock'], and endings such as -ant, -e, -ation, and -er are removed ['cormorant' to 'cormor', 'lodge' to 'lodg', 'voice' to 'voic', 'desolation' to 'desol', 'uncover' to 'uncov']\n",
    "\n",
    "THE POSTGRESQL USES STEMMING\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 4) Identify the differences and/or similarities in the \n",
    "# lemmatization of the `line` variable.\n",
    "\n",
    "After lemmatization of this particular line with stop words removed, only the plurals are turned to the singular. The other changes done by stemming are not performed.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some Queries against the loaded books.\n",
    "\n",
    "  1. In the cells below, try a couple queries, until you find an interesting line in the results.\n",
    "  1. Then copy and paste that line into the `line` variable in the appropriate cell below.\n",
    "  1. Compare the vectorization of the line to PostgreSQL.\n",
    "  1. Answer the question\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: dsa_ro_user@dsa_student'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext sql\n",
    "%sql postgres://dsa_ro_user:readonly@pgsql.dsa.lan/dsa_student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Fill in \"`<write your query here>`\" with a query or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://dsa_ro_user:***@pgsql.dsa.lan/dsa_student\n",
      "20 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>id</th>\n",
       "        <th>name</th>\n",
       "        <th>line_no</th>\n",
       "        <th>line</th>\n",
       "        <th>rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>11025</td>\n",
       "        <td>/dsa/data/all_datasets/book/ezekiel.txt</td>\n",
       "        <td>264</td>\n",
       "        <td>13:19: And will ye pollute me among my people for handfuls of barley and for pieces of bread, to slay the souls that should not die, and to save the souls alive that should not live, by your lying to my people that hear your lies?</td>\n",
       "        <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>21458</td>\n",
       "        <td>/dsa/data/all_datasets/book/levit.txt</td>\n",
       "        <td>533</td>\n",
       "        <td>18:23: Neither shalt thou lie with any beast to defile thyself therewith: neither shall any woman stand before a beast to lie down thereto: it is confusion.</td>\n",
       "        <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>27145</td>\n",
       "        <td>/dsa/data/all_datasets/book/proverbs.txt</td>\n",
       "        <td>378</td>\n",
       "        <td>14:5: A faithful witness will not lie: but a false witness will utter lies.</td>\n",
       "        <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>10831</td>\n",
       "        <td>/dsa/data/all_datasets/book/ezekiel.txt</td>\n",
       "        <td>70</td>\n",
       "        <td>4:4: Lie thou also upon thy left side, and lay the iniquity of the house of Israel upon it: according to the number of the days that thou shalt lie upon it thou shalt bear their iniquity.</td>\n",
       "        <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>31250</td>\n",
       "        <td>/dsa/data/all_datasets/book/zeph.txt</td>\n",
       "        <td>48</td>\n",
       "        <td>3:13: The remnant of Israel shall not do iniquity, nor speak lies; neither shall a deceitful tongue be found in their mouth: for they shall feed and lie down, and none shall make them afraid. </td>\n",
       "        <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1669</td>\n",
       "        <td>/dsa/data/all_datasets/book/1john.txt</td>\n",
       "        <td>32</td>\n",
       "        <td> 2:21: I have not written unto you because ye know not the truth, but because ye know it, and that no lie is of the truth.</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1644</td>\n",
       "        <td>/dsa/data/all_datasets/book/1john.txt</td>\n",
       "        <td>7</td>\n",
       "        <td> 1:6: If we say that we have fellowship with him, and walk in darkness, we lie, and do not the truth:</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1116</td>\n",
       "        <td>/dsa/data/all_datasets/book/ruth.txt</td>\n",
       "        <td>53</td>\n",
       "        <td>3:7: And when Boaz had eaten and drunk, and his heart was merry, he went to lie down at the end of the heap of corn: and she came softly, and uncovered his feet, and laid her down.</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2530</td>\n",
       "        <td>/dsa/data/all_datasets/book/1kings.txt</td>\n",
       "        <td>786</td>\n",
       "        <td>22:22: And the LORD said unto him, Wherewith?  And he said, I will go forth, and I will be a lying spirit in the mouth of all his prophets.  And he said, Thou shalt persuade him, and prevail also: go forth, and do so.</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2230</td>\n",
       "        <td>/dsa/data/all_datasets/book/1kings.txt</td>\n",
       "        <td>486</td>\n",
       "        <td>13:18: He said unto him, I am a prophet also as thou art; and an angel spake unto me by the word of the LORD, saying, Bring him back with thee into thine house, that he may eat bread and drink water.  But he lied unto him. </td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2531</td>\n",
       "        <td>/dsa/data/all_datasets/book/1kings.txt</td>\n",
       "        <td>787</td>\n",
       "        <td>22:23: Now therefore, behold, the LORD hath put a lying spirit in the mouth of all these thy prophets, and the LORD hath spoken evil concerning thee.</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2738</td>\n",
       "        <td>/dsa/data/all_datasets/book/1samuel.txt</td>\n",
       "        <td>70</td>\n",
       "        <td>3:5: And he ran unto Eli, and said, Here am I; for thou calledst me.  And he said, I called not; lie down again.  And he went and lay down. </td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2739</td>\n",
       "        <td>/dsa/data/all_datasets/book/1samuel.txt</td>\n",
       "        <td>71</td>\n",
       "        <td>3:6: And the LORD called yet again, Samuel.  And Samuel arose and went to Eli, and said, Here am I; for thou didst call me.  And he answered, I called not, my son; lie down again.</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2742</td>\n",
       "        <td>/dsa/data/all_datasets/book/1samuel.txt</td>\n",
       "        <td>74</td>\n",
       "        <td>3:9: Therefore Eli said unto Samuel, Go, lie down: and it shall be, if he call thee, that thou shalt say, Speak, LORD; for thy servant heareth.  So Samuel went and lay down in his place.</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1592</td>\n",
       "        <td>/dsa/data/all_datasets/book/titus.txt</td>\n",
       "        <td>3</td>\n",
       "        <td>1:2: In hope of eternal life, which God, that cannot lie, promised before the world began;</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3598</td>\n",
       "        <td>/dsa/data/all_datasets/book/1timothy.txt</td>\n",
       "        <td>28</td>\n",
       "        <td>2:7: Whereunto I am ordained a preacher, and an apostle, (I speak the truth in Christ, and lie not;) a teacher of the Gentiles in faith and verity.</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3257</td>\n",
       "        <td>/dsa/data/all_datasets/book/1samuel.txt</td>\n",
       "        <td>589</td>\n",
       "        <td>22:13: And Saul said unto him, Why have ye conspired against me, thou and the son of Jesse, in that thou hast given him bread, and a sword, and hast enquired of God for him, that he should rise against me, to lie in wait, as at this day?</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1747</td>\n",
       "        <td>/dsa/data/all_datasets/book/1kings.txt</td>\n",
       "        <td>3</td>\n",
       "        <td>1:2: Wherefore his servants said unto him, Let there be sought for my lord the king a young virgin: and let her stand before the king, and let her cherish him, and let her lie in thy bosom, that my lord the king may get heat. </td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3624</td>\n",
       "        <td>/dsa/data/all_datasets/book/1timothy.txt</td>\n",
       "        <td>54</td>\n",
       "        <td>4:2: Speaking lies in hypocrisy; having their conscience seared with a hot iron;</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3252</td>\n",
       "        <td>/dsa/data/all_datasets/book/1samuel.txt</td>\n",
       "        <td>584</td>\n",
       "        <td>22:8: That all of you have conspired against me, and there is none that sheweth me that my son hath made a league with the son of Jesse, and there is none of you that is sorry for me, or sheweth unto me that my son hath stirred up my servant against me, to lie in wait, as at this day?</td>\n",
       "        <td>0.1</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(11025, '/dsa/data/all_datasets/book/ezekiel.txt', 264, '13:19: And will ye pollute me among my people for handfuls of barley and for pieces of bread, to slay the souls that should not die, and to save the souls alive that should not live, by your lying to my people that hear your lies?', 0.2),\n",
       " (21458, '/dsa/data/all_datasets/book/levit.txt', 533, '18:23: Neither shalt thou lie with any beast to defile thyself therewith: neither shall any woman stand before a beast to lie down thereto: it is confusion.', 0.2),\n",
       " (27145, '/dsa/data/all_datasets/book/proverbs.txt', 378, '14:5: A faithful witness will not lie: but a false witness will utter lies.', 0.2),\n",
       " (10831, '/dsa/data/all_datasets/book/ezekiel.txt', 70, '4:4: Lie thou also upon thy left side, and lay the iniquity of the house of Israel upon it: according to the number of the days that thou shalt lie upon it thou shalt bear their iniquity.', 0.2),\n",
       " (31250, '/dsa/data/all_datasets/book/zeph.txt', 48, '3:13: The remnant of Israel shall not do iniquity, nor speak lies; neither shall a deceitful tongue be found in their mouth: for they shall feed and lie down, and none shall make them afraid. ', 0.2),\n",
       " (1669, '/dsa/data/all_datasets/book/1john.txt', 32, ' 2:21: I have not written unto you because ye know not the truth, but because ye know it, and that no lie is of the truth.', 0.1),\n",
       " (1644, '/dsa/data/all_datasets/book/1john.txt', 7, ' 1:6: If we say that we have fellowship with him, and walk in darkness, we lie, and do not the truth:', 0.1),\n",
       " (1116, '/dsa/data/all_datasets/book/ruth.txt', 53, '3:7: And when Boaz had eaten and drunk, and his heart was merry, he went to lie down at the end of the heap of corn: and she came softly, and uncovered his feet, and laid her down.', 0.1),\n",
       " (2530, '/dsa/data/all_datasets/book/1kings.txt', 786, '22:22: And the LORD said unto him, Wherewith?  And he said, I will go forth, and I will be a lying spirit in the mouth of all his prophets.  And he said, Thou shalt persuade him, and prevail also: go forth, and do so.', 0.1),\n",
       " (2230, '/dsa/data/all_datasets/book/1kings.txt', 486, '13:18: He said unto him, I am a prophet also as thou art; and an angel spake unto me by the word of the LORD, saying, Bring him back with thee into thine house, that he may eat bread and drink water.  But he lied unto him. ', 0.1),\n",
       " (2531, '/dsa/data/all_datasets/book/1kings.txt', 787, '22:23: Now therefore, behold, the LORD hath put a lying spirit in the mouth of all these thy prophets, and the LORD hath spoken evil concerning thee.', 0.1),\n",
       " (2738, '/dsa/data/all_datasets/book/1samuel.txt', 70, '3:5: And he ran unto Eli, and said, Here am I; for thou calledst me.  And he said, I called not; lie down again.  And he went and lay down. ', 0.1),\n",
       " (2739, '/dsa/data/all_datasets/book/1samuel.txt', 71, '3:6: And the LORD called yet again, Samuel.  And Samuel arose and went to Eli, and said, Here am I; for thou didst call me.  And he answered, I called not, my son; lie down again.', 0.1),\n",
       " (2742, '/dsa/data/all_datasets/book/1samuel.txt', 74, '3:9: Therefore Eli said unto Samuel, Go, lie down: and it shall be, if he call thee, that thou shalt say, Speak, LORD; for thy servant heareth.  So Samuel went and lay down in his place.', 0.1),\n",
       " (1592, '/dsa/data/all_datasets/book/titus.txt', 3, '1:2: In hope of eternal life, which God, that cannot lie, promised before the world began;', 0.1),\n",
       " (3598, '/dsa/data/all_datasets/book/1timothy.txt', 28, '2:7: Whereunto I am ordained a preacher, and an apostle, (I speak the truth in Christ, and lie not;) a teacher of the Gentiles in faith and verity.', 0.1),\n",
       " (3257, '/dsa/data/all_datasets/book/1samuel.txt', 589, '22:13: And Saul said unto him, Why have ye conspired against me, thou and the son of Jesse, in that thou hast given him bread, and a sword, and hast enquired of God for him, that he should rise against me, to lie in wait, as at this day?', 0.1),\n",
       " (1747, '/dsa/data/all_datasets/book/1kings.txt', 3, '1:2: Wherefore his servants said unto him, Let there be sought for my lord the king a young virgin: and let her stand before the king, and let her cherish him, and let her lie in thy bosom, that my lord the king may get heat. ', 0.1),\n",
       " (3624, '/dsa/data/all_datasets/book/1timothy.txt', 54, '4:2: Speaking lies in hypocrisy; having their conscience seared with a hot iron;', 0.1),\n",
       " (3252, '/dsa/data/all_datasets/book/1samuel.txt', 584, '22:8: That all of you have conspired against me, and there is none that sheweth me that my son hath made a league with the son of Jesse, and there is none of you that is sorry for me, or sheweth unto me that my son hath stirred up my servant against me, to lie in wait, as at this day?', 0.1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT id,name,line_no,line, ts_rank_cd(line_tsv_gin, query) AS rank\n",
    "FROM dlfy6.booklines, plainto_tsquery('lie') query\n",
    "WHERE query @@ line_tsv_gin\n",
    "ORDER BY rank DESC LIMIT 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['22', 'conspired', 'sheweth', 'son', 'hath', 'league', 'son', 'jesse', 'sorry', 'sheweth', 'unto', 'son', 'hath', 'stirred', 'servant', 'lie', 'wait', 'day']\n",
      " \n",
      "Stem: \n",
      "['22', 'conspir', 'sheweth', 'son', 'hath', 'leagu', 'son', 'jess', 'sorri', 'sheweth', 'unto', 'son', 'hath', 'stir', 'servant', 'lie', 'wait', 'day']\n",
      " \n",
      "Lemma: \n",
      "['22', 'conspired', 'sheweth', 'son', 'hath', 'league', 'son', 'jesse', 'sorry', 'sheweth', 'unto', 'son', 'hath', 'stirred', 'servant', 'lie', 'wait', 'day']\n"
     ]
    }
   ],
   "source": [
    "line = \"22:8: That all of you have conspired against me, and there is none that sheweth me that my son hath made a league with the son of Jesse, and there is none of you that is sorry for me, or sheweth unto me that my son hath stirred up my servant against me, to lie in wait, as at this day?\"\n",
    "\n",
    "# 2) Tokenize, Stem, and/or Lemmatize with Python\n",
    "#---------------------------------------------------------\n",
    "\n",
    "line_tokens = tk_function(line)\n",
    "print(line_tokens)\n",
    "print(' ')\n",
    "stem = []\n",
    "stemmer = PorterStemmer()\n",
    "for w in line_tokens:\n",
    "    stem.append(stemmer.stem(w))\n",
    "\n",
    "print('Stem: ')\n",
    "print(stem)\n",
    "\n",
    "print(' ')\n",
    "print('Lemma: ')\n",
    "\n",
    "lemma = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w in line_tokens:\n",
    "    lemma.append(lemmatizer.lemmatize(w))\n",
    "print(lemma)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing stop words\n",
      "----------------------------------------\n",
      "['22:8', 'conspired', 'none', 'sheweth', 'son', 'hath', 'made', 'league', 'son', 'jesse', 'none', 'sorry', 'sheweth', 'unto', 'son', 'hath', 'stirred', 'servant', 'lie', 'wait', 'day']\n",
      "\n",
      " After Stemming\n",
      "----------------------------------------\n",
      "['22:8', 'conspir', 'none', 'sheweth', 'son', 'hath', 'made', 'leagu', 'son', 'jess', 'none', 'sorri', 'sheweth', 'unto', 'son', 'hath', 'stir', 'servant', 'lie', 'wait', 'day']\n",
      "\n",
      " After Lemmatization\n",
      "----------------------------------------\n",
      "['22:8', 'conspired', 'none', 'sheweth', 'son', 'hath', 'made', 'league', 'son', 'jesse', 'none', 'sorry', 'sheweth', 'unto', 'son', 'hath', 'stirred', 'servant', 'lie', 'wait', 'day']\n"
     ]
    }
   ],
   "source": [
    "# OR OR OR \n",
    "# 2) Tokenize, Stem, and/or Lemmatize with Python\n",
    "#---------------------------------------------------------\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "tokens = [token for token in word_tokenize(line.lower()) if token not in stop]\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "print(\"After removing stop words\")\n",
    "print(40*'-')\n",
    "print(tokens)\n",
    "\n",
    "stems=[]\n",
    "for token in tokens:\n",
    "    stems.append(stemmer.stem(token))\n",
    "print(\"\\n After Stemming\")\n",
    "print(40*'-')\n",
    "print(stems)\n",
    "\n",
    "lemmas=[]\n",
    "print(\"\\n After Lemmatization\")\n",
    "print(40*'-')\n",
    "for w in tokens:\n",
    "    lemmas.append(lemmatizer.lemmatize(w))\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Now look up the search vector that PostgreSQL built by the ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgres://dsa_ro_user:***@pgsql.dsa.lan/dsa_student\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>id</th>\n",
       "        <th>name</th>\n",
       "        <th>line_no</th>\n",
       "        <th>line</th>\n",
       "        <th>line_tsv_gin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3252</td>\n",
       "        <td>/dsa/data/all_datasets/book/1samuel.txt</td>\n",
       "        <td>584</td>\n",
       "        <td>22:8: That all of you have conspired against me, and there is none that sheweth me that my son hath made a league with the son of Jesse, and there is none of you that is sorry for me, or sheweth unto me that my son hath stirred up my servant against me, to lie in wait, as at this day?</td>\n",
       "        <td>&#x27;22&#x27;:1 &#x27;8&#x27;:2 &#x27;conspir&#x27;:8 &#x27;day&#x27;:62 &#x27;hath&#x27;:21,48 &#x27;jess&#x27;:29 &#x27;leagu&#x27;:24 &#x27;lie&#x27;:56 &#x27;made&#x27;:22 &#x27;none&#x27;:14,33 &#x27;servant&#x27;:52 &#x27;sheweth&#x27;:16,42 &#x27;son&#x27;:20,27,47 &#x27;sorri&#x27;:38 &#x27;stir&#x27;:49 &#x27;unto&#x27;:43 &#x27;wait&#x27;:58</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(3252, '/dsa/data/all_datasets/book/1samuel.txt', 584, '22:8: That all of you have conspired against me, and there is none that sheweth me that my son hath made a league with the son of Jesse, and there is none of you that is sorry for me, or sheweth unto me that my son hath stirred up my servant against me, to lie in wait, as at this day?', \"'22':1 '8':2 'conspir':8 'day':62 'hath':21,48 'jess':29 'leagu':24 'lie':56 'made':22 'none':14,33 'servant':52 'sheweth':16,42 'son':20,27,47 'sorri':38 'stir':49 'unto':43 'wait':58\")]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT id,name,line_no,line, line_tsv_gin\n",
    "FROM dlfy6.booklines\n",
    "WHERE id = 3252;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 4) You have seen the vector representation in PostgreSQL a few times now.\n",
    "#  Please contemplate the numbers after the terms. \n",
    "#  Hypothesize why they are there based on your inspection of input versus vector.\n",
    "#  Estimate a use-case or reason to have the numbers coded within the vector.\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "The numbers after the terms represent the location for that term in the line. \n",
    "\n",
    "Terms could be evaluated for proximity to each other.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE YOUR NOTEBOOK, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
