{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Building and Loading Text Search in Python Whoosh\n",
    "\n",
    "\n",
    "## OUTLINE\n",
    " 1. [Whoosh](#Whoosh_text)\n",
    " 1. [Task at hand](#task)\n",
    " 1. [Buiding our Whoosh Schema](#build_it)\n",
    " 1. [Loading Data](#load_it)\n",
    " 1. [Executing Queries, Google-lite...very very lite](#search_me) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='Whoosh_text' ></a>\n",
    "\n",
    "## Whoosh\n",
    "\n",
    "Whoosh was started as a quick and dirty search server for the online documentation of the Houdini 3D animation software package. \n",
    "Side Effects Software generously allowed the code to be open source, in case it might be useful to anyone else who needs a very flexible or pure-Python search engine (or both!).\n",
    "\n",
    "  * Whoosh is fast, but uses only pure Python, so it will run anywhere Python runs, without requiring a compiler.\n",
    "  * By default, Whoosh uses the Okapi BM25F ranking function, but like most things the ranking function can be easily customized.\n",
    "  * Whoosh creates fairly small indexes compared to many other search libraries.\n",
    "  * All indexed text in Whoosh must be unicode.\n",
    "  * Whoosh lets you store arbitrary Python objects with indexed documents.\n",
    "\n",
    "### What is Whoosh?\n",
    "\n",
    "Whoosh is a fast, pure Python search engine library.\n",
    "\n",
    "The primary design impetus of Whoosh is that it is pure Python. \n",
    "You should be able to use Whoosh anywhere you can use Python, no compiler or Java required.\n",
    "\n",
    "Like one of its ancestors, Lucene, Whoosh is not really a search engine, itâ€™s a programmer library for creating a search engine.\n",
    "\n",
    "Practically no important behavior of Whoosh is hard-coded. \n",
    "Indexing of text, the level of information stored for each term in each field, parsing of search queries, the types of queries allowed, scoring algorithms, etc. are all customizable, replaceable, and extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='task' ></a>\n",
    "\n",
    "## Task at hand\n",
    "\n",
    "For this exercise, we are going to walk through the process of creating full text search capability within Python for integration into other analytical processes.\n",
    "\n",
    "You previously read about the _`book`_ data and you have seen the data used for a corpus in a PostgreSQL full text search, as well as using Whoosh in Python.\n",
    "\n",
    "Now, we are going go through the similar process to build a search engine in pure Python for a different corpus.\n",
    "\n",
    "The process will take very little time and the useability of the full text search is multiplied by degree of heterogeneous data that can be integrated with the full text search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='build_it' ></a>\n",
    "\n",
    "## Buiding our Whoosh Schema\n",
    "\n",
    "Recall, the `book/` folder is composed of a collection of text files, each its own book chapter.\n",
    "\n",
    "In whoosh, we structure the retrieval system by defining a storage schema.\n",
    "\n",
    "From the lab with the text files:\n",
    "```\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "                )\n",
    "```\n",
    "\n",
    "This tells us we are defining records to have a `(filename, content)`\n",
    "\n",
    "For this exercise, we will be using a few Wikipedia pages for our data source.\n",
    "\n",
    "### 1) For this exercise, you should look at a few of these web pages:\n",
    "\n",
    "  * https://en.wikipedia.org/wiki/Nyctimantis\n",
    "  * https://en.wikipedia.org/wiki/Osteocephalus\n",
    "  * https://en.wikipedia.org/wiki/Osteopilus\n",
    "  \n",
    "Specifically, inspect the HTML source and the \n",
    "```HTML\n",
    "<table class=\"infobox biota\" ... </table>\n",
    "```\n",
    "\n",
    "You need to extend the schema definition to collect the table data when available."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# change this to a code cell and run if you have trouble with a locked writer\n",
    "from whoosh import writing\n",
    "writer.commit(mergetype=writing.CLEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer()),\n",
    "                # Extend the schema definition to capture relevant table data            \n",
    "                Kingdom=TEXT(stored=True),\n",
    "                Phylum=TEXT(stored=True),\n",
    "                Class=TEXT(stored=True),\n",
    "                Order=TEXT(stored=True),\n",
    "                Family=TEXT(stored=True),\n",
    "                Subfamily=TEXT(stored=True),\n",
    "                Genus=TEXT(stored=True),\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='load_it' ></a>\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "For this exercise, we have created a small folder of a few Wikipedia pages under the `en.wikipedia.org/wiki` folder in the common datasets folder:\n",
    "\n",
    "```Bash\n",
    "sebcq5@jupyter-sebcq5:/dsa/data/all_datasets$ ls en.wikipedia.org/wiki\n",
    "Acris.html           Charadrahyla.html   Hylidae.html      Megastomatohyla.html  Plectrohyla.html  Sphaenorhynchus.html\n",
    "Anotheca.html        Corythomantis.html  Hylinae.html      Myersiohyla.html      Pseudacris.html   Tepuihyla.html\n",
    "Aparasphenodon.html  Dendropsophus.html  Hyloscirtus.html  Nyctimantis.html      Pseudis.html      Tlalocohyla.html\n",
    "Aplastodiscus.html   Duellmanohyla.html  Hypsiboas.html    Osteocephalus.html    Ptychohyla.html   Trachycephalus.html\n",
    "Argenteohyla.html    Ecnomiohyla.html    Isthmohyla.html   Osteopilus.html       Scarthyla.html    Triprion.html\n",
    "Bokermannohyla.html  Exerodonta.html     Itapotihyla.html  Phyllodytes.html      Scinax.html       Xenohyla.html\n",
    "Bromeliohyla.html    Hyla.html           Lysapsus.html     Phytotriades.html     Smilisca.html\n",
    "\n",
    "```\n",
    "You will create the _whoosh_ index files in the modules/module5/exercises/wiki_index folder then ingest the files.\n",
    "\n",
    "To load the data, a python script with follow the basic crawling behavior\n",
    "\n",
    " 1. For each file/folder in the specified starting folder:\n",
    " 1. If it is a folder, recurse into folder and process contents\n",
    " 1. If it is a file, read contents and load into indexer.\n",
    " \n",
    "## Follow the lab for Python IR with whoosh to complete this exercise.\n",
    "\n",
    "### Step 2) Create / Initialize the whoosh index and get the `writer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from whoosh import index\n",
    "\n",
    "# Step 2 below this comment\"\n",
    "ix = index.create_in('wiki_index', schema)\n",
    "\n",
    "writer = ix.writer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Adapt the helper functions\n",
    "\n",
    "Note the subtle changes.\n",
    "Please adapt the code below such as provided recursive parsing of the HTML (.html) files, indexing with the Whoosh API.\n",
    "Trust no code, verify all code segments.\n",
    "\n",
    "HINT: When parsing the `\"<table class=\"infobox biota\" ... </table>` data, consider the difference between `.string` and `.get_text()` and experiment to see if it makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element)):\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullBiota(content):\n",
    "        \n",
    "    data = {}\n",
    "                    \n",
    "    # TODO: Process the \"<table class=\"infobox biota\" ... </table> data\n",
    "    soup = BeautifulSoup(content, 'html.parser')    \n",
    "\n",
    "    table = soup.findAll('table', attrs = {'class': re.compile(r'\\binfobox\\b')})\n",
    "        \n",
    "    table_as_bs = BeautifulSoup(str(table), 'html.parser')\n",
    "        \n",
    "    for row in table_as_bs.findAll('tr'):\n",
    "        cells = row.findAll('td')\n",
    "        if(len(cells)==2):\n",
    "            #print(cells[0].string,'-',cells[1].string)\n",
    "            #print(cells[0].string,'-',cells[1].get_text())\n",
    "            #print(cells[0].string,'-',cells[1].get_text().split('\\n', 1))\n",
    "            #print(cells[0].string,'-',cells[1].get_text().split('\\n', 1)[0])\n",
    "            if (cells[1].get_text()):\n",
    "                data[cells[0].string.strip(':')]=cells[1].get_text().split('\\n', 1)[0]\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    with open(fname, 'r', encoding=\"utf-8\") as infile:\n",
    "        #visible_texts=infile.read()\n",
    "        content=infile.read()\n",
    "        \n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')    \n",
    "\n",
    "        texts = soup.findAll(text=True)\n",
    "        \n",
    "        # Process all the visible text\n",
    "        visible_texts = filter(visible, texts)\n",
    "        \n",
    "        # TODO: Assemble all visible_texts into a content string\n",
    "        page_text = \"\"\n",
    "        for line in visible_texts:\n",
    "            page_text += \" \" + line.strip (\"\\n\")\n",
    "#         print(\"-----------------\")\n",
    "#         print(page_text)\n",
    "#         print(\"-----------------\")\n",
    "    \n",
    "        # TODO: Process the \"<table class=\"infobox biota\" ... </table> data\n",
    "        tableOut = pullBiota(content)\n",
    "        #print(tableOut)\n",
    "\n",
    "        # Write to the index\n",
    "        writer.add_document(filename=fname,\n",
    "                            content=page_text,\n",
    "                            Kingdom=tableOut.get('Kingdom'),\n",
    "                            Phylum=tableOut.get('Phylum'),\n",
    "                            Class=tableOut.get('Class'),\n",
    "                            Order=tableOut.get('Order'),\n",
    "                            Family=tableOut.get('Family'),\n",
    "                            Subfamily=tableOut.get('Subfamily'),\n",
    "                            Genus=tableOut.get('Genus')\n",
    "                           )\n",
    "        print(\"Indexed: \",fname)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Plectrohyla.html\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "# loadFile(writer,'./wiki_index/Plectrohyla.html')\n",
    "loadFile(writer,'/dsa/data/all_datasets/en.wikipedia.org/wiki/Plectrohyla.html')\n",
    "writer.commit() # save changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Plectrohyla', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Plectrohyla.html'}>\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(u\"frog\")                             \n",
    "\n",
    "with ix.searcher() as s:\n",
    "     results = s.search(q)\n",
    "     for hit in results:\n",
    "         print(hit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFolder(writer,folder):\n",
    "    '''\n",
    "    Process a folder for files and subfolders\n",
    "    '''\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        print(\"root = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".html\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File: \",file)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Parse with our defined functions in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder:  /dsa/data/all_datasets/en.wikipedia.org/wiki\n",
      "root =  /dsa/data/all_datasets/en.wikipedia.org/wiki\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Acris.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Acris.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Anotheca.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Anotheca.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Aparasphenodon.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Aparasphenodon.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Aplastodiscus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Aplastodiscus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Argenteohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Argenteohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Bokermannohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Bokermannohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Bromeliohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Bromeliohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Charadrahyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Charadrahyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Corythomantis.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Corythomantis.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Dendropsophus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Dendropsophus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Duellmanohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Duellmanohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Ecnomiohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Ecnomiohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Exerodonta.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Exerodonta.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Hyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Hyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Hylidae.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Hylidae.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Hylinae.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Hylinae.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Hyloscirtus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Hyloscirtus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Hypsiboas.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Hypsiboas.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Isthmohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Isthmohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Itapotihyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Itapotihyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Lysapsus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Lysapsus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Megastomatohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Megastomatohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Myersiohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Myersiohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Nyctimantis.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Nyctimantis.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Osteocephalus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Osteocephalus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Osteopilus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Osteopilus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Phyllodytes.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Phyllodytes.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Phytotriades.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Phytotriades.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Plectrohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Plectrohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudacris.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudacris.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudis.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudis.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Ptychohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Ptychohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Scarthyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Scarthyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Scinax.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Scinax.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Smilisca.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Smilisca.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Sphaenorhynchus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Sphaenorhynchus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Tepuihyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Tepuihyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Tlalocohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Tlalocohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Trachycephalus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Trachycephalus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Triprion.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Triprion.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Xenohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Xenohyla.html\n"
     ]
    }
   ],
   "source": [
    "# Start processing the folder and commit the work\n",
    "# ---------------------------------------------------\n",
    "import sys\n",
    "sys.setrecursionlimit(1000)\n",
    "\n",
    "# Functions defined,  get the party started:\n",
    "processFolder(writer, '/dsa/data/all_datasets/en.wikipedia.org/wiki')\n",
    "writer.commit() # save changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='search_me' ></a>\n",
    "\n",
    "### 5) Executing Queries\n",
    "\n",
    "Read: \n",
    "  http://whoosh.readthedocs.io/en/latest/searching.html\n",
    "  \n",
    "Previously, we hard-coded query strings into the code cells.\n",
    "\n",
    "Now, use the `input()` function collect a query string from the user. \n",
    "Then execute the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are you searching for? frog\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "val = input(\"What are you searching for? \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Dendropsophus', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Dendropsophus.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Hypsiboas', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Hypsiboas.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Hyla', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Hyla.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Hylidae.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Plectrohyla', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Plectrohyla.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Pseudacris', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudacris.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Hylinae.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Hyloscirtus', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Hyloscirtus.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Ptychohyla', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Ptychohyla.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Bokermannohyla', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Bokermannohyla.html'}>\n"
     ]
    }
   ],
   "source": [
    "val = 'u\"' + str(val) + '\"'\n",
    "qp = QueryParser(\"content\",schema=ix.schema)\n",
    "q = qp.parse(val)\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    print(\"Found {}\".format(len(results)))\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Write example queries to ensure you can search the index \n",
    "\n",
    "That is, make sure you can search on the fields you added to the index from the infobox biota table.\n",
    "\n",
    "```HTML\n",
    "<table class=\"infobox biota\" ... </table>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search for: Ptychohyla\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ptychohyla'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "# example search : Ptychohyla in any field, including content\n",
    "phrase = input(\"search for: \")\n",
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Ptychohyla', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Ptychohyla.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Hylinae.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Hylidae.html'}>\n"
     ]
    }
   ],
   "source": [
    "from whoosh import qparser\n",
    "\n",
    "qp = qparser.MultifieldParser([\"content\",\"Kingdom\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\"], \n",
    "                              schema=ix.schema,group=qparser.OrGroup)\n",
    "q = qp.parse(phrase)\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    print(\"Found {}\".format(len(results)))\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search for: Anura\n",
      "<Top 10 Results for Or([Term('Kingdom', 'anura'), Term('Phylum', 'anura'), Term('Class', 'anura'), Term('Order', 'anura'), Term('Family', 'anura'), Term('Genus', 'anura')]) runtime=0.0014877889771014452>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Acris', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Acris.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Anotheca', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Anotheca.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Aparasphenodon', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Aparasphenodon.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Aplastodiscus', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Aplastodiscus.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Argenteohyla', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Argenteohyla.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Bokermannohyla', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Bokermannohyla.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Bromeliohyla', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Bromeliohyla.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Charadrahyla', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Charadrahyla.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Corythomantis', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Corythomantis.html'}>\n",
      "<Hit {'Class': 'Amphibia', 'Family': 'Hylidae', 'Genus': 'Dendropsophus', 'Kingdom': 'Animalia', 'Order': 'Anura', 'Phylum': 'Chordata', 'Subfamily': 'Hylinae', 'filename': '/dsa/data/all_datasets/en.wikipedia.org/wiki/Dendropsophus.html'}>\n"
     ]
    }
   ],
   "source": [
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "# example search for Anura, an order\n",
    "phrase = input(\"search for: \")\n",
    "phrase\n",
    "\n",
    "# OMIT CONTENT\n",
    "qp = qparser.MultifieldParser([\"Kingdom\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\"], schema=ix.schema,group=qparser.OrGroup)\n",
    "q = qp.parse(phrase)\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    print(results)\n",
    "    for hit in results:\n",
    "        print(hit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE YOUR NOTEBOOK WITH ALL EXECUTED CELLS\n",
    "# Then, `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
